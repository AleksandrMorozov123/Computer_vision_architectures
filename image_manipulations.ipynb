{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aleksandrmorozov123/image-manipulations?scriptVersionId=231731350\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-04-04T04:14:10.441245Z","iopub.execute_input":"2025-04-04T04:14:10.442057Z","iopub.status.idle":"2025-04-04T04:14:10.748894Z","shell.execute_reply.started":"2025-04-04T04:14:10.442023Z","shell.execute_reply":"2025-04-04T04:14:10.748274Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**Convolutional auto-encoder**","metadata":{}},{"cell_type":"code","source":"!pip install -q torch_snippets\n!pip install torchsummary\n\nimport torch\nimport torch.nn as nn\nfrom torch_snippets import *\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import FashionMNIST\nfrom torchvision import transforms\nfrom torchsummary import summary\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nimg_transform = transforms.Compose ([\n    transforms.ToTensor (),\n    transforms.Normalize ([0.5], [0.5]),\n    transforms.Lambda (lambda x: x.to(device))\n])\n\ntrn_ds = FashionMNIST ('/content/', transform = img_transform, train = True, download = True)\nval_ds = FashionMNIST ('/content/', transform = img_transform, train = False, download = True)\n\nbatch_size = 128\ntrn_dl = DataLoader (trn_ds, batch_size = batch_size, shuffle = True)\nval_dl = DataLoader (val_ds, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:27:50.299276Z","iopub.execute_input":"2025-04-04T04:27:50.300214Z","iopub.status.idle":"2025-04-04T04:28:07.35712Z","shell.execute_reply.started":"2025-04-04T04:27:50.300177Z","shell.execute_reply":"2025-04-04T04:28:07.355981Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Error parsing requirements for executing: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/executing-2.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0m\u001b[33m    WARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Cannot uninstall executing 2.0.1, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps executing==2.0.1'.\u001b[0m\u001b[31m\n\u001b[0mCollecting torchsummary\n  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\nDownloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n\u001b[33mWARNING: Error parsing requirements for executing: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/executing-2.0.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: torchsummary\nSuccessfully installed torchsummary-1.5.1\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class ConvAutoEncoder (nn.Module):\n    def __init__ (self):\n        super ().__init__()\n        self.encoder = nn.Sequential (\n            nn.Conv2d (1, 32, 3, stride = 3, padding = 1), nn.ReLU(True),\n            nn.MaxPool2d (2, stride = 2),\n            nn.Conv2d (32, 64, 3, stride = 2, padding = 1), nn.ReLU (True),\n            nn.MaxPool2d (2, stride = 1)\n        )\n        self.decoder = nn.Sequential (\n            nn.ConvTranspose2d (64, 32, 3, stride = 2), nn.ReLU (True),\n            nn.ConvTranspose2d (32, 16, 5, stride = 3, padding = 1), nn.ReLU(True),\n            nn.ConvTranspose2d (16, 1, 2, stride = 2, padding = 1), nn.Tanh ()\n        )\n    def forward (self, x):\n         x = self.encoder (x)\n         x = self.decoder (x)\n         return x\n        \nmodel = ConvAutoEncoder().to(device)\n\nsummary (model, torch.zeros (2, 1, 28, 28))\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:19:27.975021Z","iopub.execute_input":"2025-04-04T04:19:27.975914Z","iopub.status.idle":"2025-04-04T04:19:28.677916Z","shell.execute_reply.started":"2025-04-04T04:19:27.975874Z","shell.execute_reply":"2025-04-04T04:19:28.677023Z"}},"outputs":[{"name":"stdout","text":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [-1, 64, 2, 2]            --\n|    └─Conv2d: 2-1                       [-1, 32, 10, 10]          320\n|    └─ReLU: 2-2                         [-1, 32, 10, 10]          --\n|    └─MaxPool2d: 2-3                    [-1, 32, 5, 5]            --\n|    └─Conv2d: 2-4                       [-1, 64, 3, 3]            18,496\n|    └─ReLU: 2-5                         [-1, 64, 3, 3]            --\n|    └─MaxPool2d: 2-6                    [-1, 64, 2, 2]            --\n├─Sequential: 1-2                        [-1, 1, 28, 28]           --\n|    └─ConvTranspose2d: 2-7              [-1, 32, 5, 5]            18,464\n|    └─ReLU: 2-8                         [-1, 32, 5, 5]            --\n|    └─ConvTranspose2d: 2-9              [-1, 16, 15, 15]          12,816\n|    └─ReLU: 2-10                        [-1, 16, 15, 15]          --\n|    └─ConvTranspose2d: 2-11             [-1, 1, 28, 28]           65\n|    └─Tanh: 2-12                        [-1, 1, 28, 28]           --\n==========================================================================================\nTotal params: 50,161\nTrainable params: 50,161\nNon-trainable params: 0\nTotal mult-adds (M): 3.64\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.07\nParams size (MB): 0.19\nEstimated Total Size (MB): 0.27\n==========================================================================================\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\n├─Sequential: 1-1                        [-1, 64, 2, 2]            --\n|    └─Conv2d: 2-1                       [-1, 32, 10, 10]          320\n|    └─ReLU: 2-2                         [-1, 32, 10, 10]          --\n|    └─MaxPool2d: 2-3                    [-1, 32, 5, 5]            --\n|    └─Conv2d: 2-4                       [-1, 64, 3, 3]            18,496\n|    └─ReLU: 2-5                         [-1, 64, 3, 3]            --\n|    └─MaxPool2d: 2-6                    [-1, 64, 2, 2]            --\n├─Sequential: 1-2                        [-1, 1, 28, 28]           --\n|    └─ConvTranspose2d: 2-7              [-1, 32, 5, 5]            18,464\n|    └─ReLU: 2-8                         [-1, 32, 5, 5]            --\n|    └─ConvTranspose2d: 2-9              [-1, 16, 15, 15]          12,816\n|    └─ReLU: 2-10                        [-1, 16, 15, 15]          --\n|    └─ConvTranspose2d: 2-11             [-1, 1, 28, 28]           65\n|    └─Tanh: 2-12                        [-1, 1, 28, 28]           --\n==========================================================================================\nTotal params: 50,161\nTrainable params: 50,161\nNon-trainable params: 0\nTotal mult-adds (M): 3.64\n==========================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.07\nParams size (MB): 0.19\nEstimated Total Size (MB): 0.27\n=========================================================================================="},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def train_batch (input, model, criterion, optimizer):\n    model.train ()\n    optimizer.zero_grad ()\n    output = model (input)\n    loss = criterion (output, input)\n    loss.backward ()\n    optimizer.step ()\n    return loss\n\n@torch.no_grad ()\ndef validate_batch (input, model, criterion):\n    model.eval ()\n    output = model (input)\n    loss = criterion (output, input)\n    return loss\n\nmodel = ConvAutoEncoder().to(device)\ncriterion = nn.MSELoss()\noptimizer = torch.optim.AdamW (model.parameters(), lr = 0.001, weight_decay = 1e-5)\n\nnum_epochs = 5\n#log = report (num_epochs)\n\nfor epoch in range (num_epochs):\n    N = len (trn_dl)\n    for ix, (data, _) in enumerate (trn_dl):\n        loss = train_batch (data, model, criterion, optimizer)\n        #log.record (pos = (epoch + (ix + 1)/N), trn_loss = loss, end = '\\r')\n    #log.report_avgs (epoch + 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T04:23:44.982717Z","iopub.execute_input":"2025-04-04T04:23:44.983434Z","iopub.status.idle":"2025-04-04T04:24:58.512781Z","shell.execute_reply.started":"2025-04-04T04:23:44.983399Z","shell.execute_reply":"2025-04-04T04:24:58.511764Z"}},"outputs":[],"execution_count":16}]}